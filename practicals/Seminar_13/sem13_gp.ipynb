{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Z399pXhLB8n"
   },
   "source": [
    "## Gaussian Processes (GP) with GPyTorch\n",
    "\n",
    "In this notebook we are going to use [GPyTorch](https://gpytorch.ai/) library for GP modeling.\n",
    "\n",
    "Why **GPyTorch**?\n",
    "\n",
    "* State of the art GP models\n",
    "* Built on top of pytorch having all its advantages (GPU, autograd, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ccav36j9LB8o"
   },
   "source": [
    "Run the following line to install GPyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPq8yPA_LB8p"
   },
   "outputs": [],
   "source": [
    "!pip install gpytorch\n",
    "!git clone https://github.com/yeahrmek/BayesOpt_tutorial\n",
    "\n",
    "import os\n",
    "os.chdir('BayesOpt_tutorial/GP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OS7k7miwLB8r"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "from gpytorch import kernels\n",
    "\n",
    "\n",
    "import utils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_m0Urm5LB8t"
   },
   "source": [
    "Current documentation of GPyTorch library can be found [here](https://gpytorch.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_qsTOksLB8u"
   },
   "source": [
    "## Gaussian Process Regression\n",
    "\n",
    "A data set $\\left (X, \\mathbf{y} \\right ) = \\left \\{ (x_i, y_i), x_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{R} \\right \\}_{i = 1}^N$ is given.  \n",
    "\n",
    "Assumption:\n",
    "$$\n",
    "y = f(x) + \\varepsilon,\n",
    "$$\n",
    "where $f(x)$ is a Gaussian Processes and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$ is a Gaussian noise ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VE6NZbu-LB8v"
   },
   "source": [
    "Posterior distribution\n",
    "$$\n",
    "y_* | X, \\mathbf{y}, x_* \\sim \\mathcal{N}(m(x_*), \\sigma(x_*)),\n",
    "$$\n",
    "with predictive mean and variance given by\n",
    "$$\n",
    "m(x_*) = \\mathbf{k}^T \\mathbf{K}_y^{-1} \\mathbf{y} = \\sum_{i = 1}^N \\alpha_i k(x_*, x_i),\n",
    "$$\n",
    "$$\n",
    "\\sigma^2(x_*) = k(x_*, x_*) - \\mathbf{k}^T\\mathbf{K}_y^{-1}\\mathbf{k},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{k} = \\left ( k(x_*, x_1), \\ldots, k(x_*, x_N) \\right )^T\n",
    "$$\n",
    "$$\n",
    "\\mathbf{K}_y = \\|k(x_i, x_j)\\|_{i, j = 1}^N + \\sigma_n^2 \\mathbf{I}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "St2xzihCLB8v"
   },
   "source": [
    "## Building GPR model\n",
    "\n",
    "Lets fit GPR model for function $f(x) = âˆ’ \\cos(\\pi x) + \\sin(4\\pi x)$ in $[0, 1]$,\n",
    "with noise $y(x) = f(x) + \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0, 0.1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIUZOwo7LB8w"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1150061746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H08loiXYLB8z",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = torch.linspace(0.05, 0.95, N)\n",
    "y = -torch.cos(np.pi * X) + np.sin(4 * np.pi * X) + torch.randn(N) * 0.1\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X.numpy(), y.numpy(), 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95K01ZDLLB81"
   },
   "source": [
    "#### 1. Define covariance function\n",
    "\n",
    "The most popular kernel - RBF kernel (`kernels.RBFKernel`) - is defined as follows in gpytorch, $k(x, y) = \\exp\\left ( -\\dfrac{\\|x - y\\|^2}{2l^2}\\right )$,\n",
    "where $l$ is a `lengthscale`.  \n",
    "Usually, there is also a coefficient $A$ that scales the kernel.  \n",
    "In gpytorch we can add the scaling coefficient using `kernels.ScaleKernel` as follows\n",
    "```python\n",
    "kernel = kernels.RBFKernel()\n",
    "kernel = kernels.ScaleKernel(kernel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kL-xWcxLB82"
   },
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOcWhQAZLB84"
   },
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class GPRegressor(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X, y, kernel, likelihood=None):\n",
    "        if likelihood is None:\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        \n",
    "        super().__init__(X, y, likelihood)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.kernel = kernel\n",
    "        self.likelihood = likelihood\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean(x)\n",
    "        covar_x = self.kernel(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X)\n",
    "            return self.likelihood(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaRsiMPXLB86"
   },
   "source": [
    "#### 2. Create GPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TaaGSDbDLB86"
   },
   "outputs": [],
   "source": [
    "model = GPRegressor(X, y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leyV24ErLB88",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWpzsYVNLB8_"
   },
   "source": [
    "### Parameters of the covariance function\n",
    "\n",
    "Values of parameters of covariance function can be set like:  `kernel.lengthscale = 0.1`.\n",
    "\n",
    "Let's change the value of `lengthscale` parameter and see how it changes the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HztQNoVZLB8_"
   },
   "outputs": [],
   "source": [
    "def plot_kernel(kernel, xlim=None, ax=None):\n",
    "    if xlim is None:\n",
    "        xlim = [-3, 5]\n",
    "    x = torch.linspace(xlim[0], xlim[1], 100)\n",
    "    with torch.no_grad():\n",
    "        K = kernel(x, torch.ones((1))).evaluate().reshape(-1, 1)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "    ax.plot(x.numpy(), K.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMFw_A32LB9B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = kernels.RBFKernel()\n",
    "theta = np.asarray([0.2, 0.5, 1, 2, 4, 10])\n",
    "figure, axes = plt.subplots(2, 3, figsize=(8, 4))\n",
    "for t, ax in zip(theta, axes.ravel()):\n",
    "    k.lengthscale = t\n",
    "    plot_kernel(k, ax=ax)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend([t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjIoalkSLB9D"
   },
   "source": [
    "### Task\n",
    "Try to change parameters to obtain more accurate model.\n",
    "\n",
    "*If you use scale kernel (`kernel = kernels.ScaleKernel(base_kernel)`) then you can access\n",
    "the parameters of `base_kernel` like this:*\n",
    "```python\n",
    "kernel.base_kernel.lengthscale = 1\n",
    "```\n",
    "\n",
    "**Question**: based on the input points `X` can we guess the `lengthscale` approximately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRgQUnlMLB9E"
   },
   "outputs": [],
   "source": [
    "######## Your code goes here ########\n",
    "\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQFIdHBFLB9F"
   },
   "source": [
    "Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUamx-AsLB9G"
   },
   "outputs": [],
   "source": [
    "model.likelihood.noise = 0.01\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVWBY5m2LB9I"
   },
   "source": [
    "### Tuning parameters of the covariance function\n",
    "\n",
    "The parameters are tuned by maximizing likelihood. To do it just use `optimize()` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Yhn_b6mLB9I"
   },
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zV9dKJ7GLB9K",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "def train(model, X, y, n_epochs=100, lr=0.3, fix_noise_variance=None, verbose=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    if fix_noise_variance is not None:\n",
    "        model.likelihood.noise = fix_noise_variance\n",
    "        training_parameters = [p for name, p in model.named_parameters()\n",
    "                               if not name.startswith('likelihood')]\n",
    "    else:\n",
    "        training_parameters = model.parameters()\n",
    "        \n",
    "    optimizer = torch.optim.Adamax(training_parameters, lr=lr)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    with tqdm.trange(n_epochs, disable=not verbose) as bar:\n",
    "        for i in bar:\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(X)\n",
    "            loss = -mll(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "                        \n",
    "            # display progress bar\n",
    "            postfix = dict(Loss=f\"{loss.item():.3f}\",\n",
    "                           noise=f\"{model.likelihood.noise.item():.3}\")\n",
    "            \n",
    "            if (hasattr(model.kernel, 'base_kernel') and\n",
    "                hasattr(model.kernel.base_kernel, 'lengthscale')):\n",
    "                lengthscale = model.kernel.base_kernel.lengthscale\n",
    "                if lengthscale is not None:\n",
    "                    lengthscale = lengthscale.squeeze(0).detach().cpu().numpy()\n",
    "            else:\n",
    "                lengthscale = model.kernel.lengthscale\n",
    "\n",
    "            if lengthscale is not None:\n",
    "                if len(lengthscale) > 1:\n",
    "                    lengthscale_repr = [f\"{l:.3f}\" for l in lengthscale]\n",
    "                    postfix['lengthscale'] = f\"{lengthscale_repr}\"\n",
    "                else:\n",
    "                    postfix['lengthscale'] = f\"{lengthscale[0]:.3f}\"\n",
    "                \n",
    "            bar.set_postfix(postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9LjIhEcLB9L"
   },
   "outputs": [],
   "source": [
    "train(model, X, y)\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijQpbABrLB9N"
   },
   "source": [
    "### Noise variance\n",
    "\n",
    "Noise variance acts like a regularization in GP models. Larger values of noise variance lead to more smooth model.  \n",
    "\n",
    "**Task**: try to change noise variance to some large value, to some small value and plot the results.\n",
    "\n",
    "Noise variance accessed like this: `model.likelihood.noise = 1`  \n",
    "To fix it during training you can pass `fix_noise_variance` argument to `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W66n2CKYLB9O"
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wcnX3gK1LB9P"
   },
   "source": [
    "Now, let's generate more noisy data and try to fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1V3lhKaHLB9R",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "X = torch.linspace(0.05, 0.95, N)\n",
    "y = -torch.cos(np.pi * X) + torch.sin(4 * np.pi * X) + torch.randn(N) * 0.5\n",
    "\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)\n",
    "\n",
    "train(model, X, y)\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkA9dA9JLB9T"
   },
   "source": [
    "Now, let's fix noise variance to some small value and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFvISimTLB9U"
   },
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "\n",
    "model = GPRegressor(X, y, kernel)\n",
    "\n",
    "train(model, X, y, fix_noise_variance=0.1, n_epochs=200)\n",
    "\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VwwgXN3zLB9W"
   },
   "source": [
    "## Approximate multi-dimensional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrypMrNRLB9X"
   },
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    x = 0.5 * (4 * x - 2)\n",
    "    y = np.sum((1 - x[:, :-1])**2 +\n",
    "                   100 * (x[:, 1:] - x[:, :-1]**2)**2, axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lElPujlMLB9Z"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def plot_2d_func(func, n_rows=1, n_cols=1, title=None):\n",
    "    grid_size = 100\n",
    "    x_grid = np.meshgrid(np.linspace(0, 1, grid_size), np.linspace(0, 1, grid_size))\n",
    "    x_grid = np.hstack((x_grid[0].reshape(-1, 1), x_grid[1].reshape(-1, 1)))\n",
    "    y = func(x_grid)\n",
    "    fig = plt.figure(figsize=(n_cols * 6, n_rows * 6))\n",
    "    ax = fig.add_subplot(n_rows, n_cols, 1, projection='3d')\n",
    "    ax.plot_surface(x_grid[:, 0].reshape(grid_size, grid_size), x_grid[:, 1].reshape(grid_size, grid_size),\n",
    "                    y.reshape(grid_size, grid_size),\n",
    "                    cmap=cm.jet, rstride=1, cstride=1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KAAUq_4LB9d"
   },
   "source": [
    "#### Here how the function looks like in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuq11SKWLB9e"
   },
   "outputs": [],
   "source": [
    "fig = plot_2d_func(rosenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dGjhJGSLB9g"
   },
   "source": [
    "### Training set\n",
    "Note that it is 3-dimensional now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1ZWXWxNLB9g"
   },
   "outputs": [],
   "source": [
    "dim = 3\n",
    "X = torch.tensor(np.random.rand(300, dim), dtype=torch.float64)\n",
    "y = torch.from_numpy(rosenbrock(X.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xNyeQQULB9i"
   },
   "source": [
    "### Task\n",
    "\n",
    "Try to approximate Rosenbrock function using RBF kernel. MSE (mean squared error) should be $<10^{-2}$.\n",
    "**Hint**: if results are not good maybe it is due to bad local minimum. You can do one of the following things:\n",
    "0. Use `double()` for all evaluations\n",
    "1. Try to use different initial values of hyperparameters.\n",
    "2. Constrain model parameters to some reasonable bounds. You can do it for example as follows: \n",
    "\n",
    "```\n",
    "constraint = gpytorch.constraints.Interval(0.0, 1.0)\n",
    "kernel = kernels.RBFKernel(lengthscale_constraint=constraint)\n",
    "```\n",
    "3. What about scaling of the data? Does it affect kernel matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9Sp_0EMLB9j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmGkY4kmLB9k"
   },
   "outputs": [],
   "source": [
    "print(f\"Outputscale: {model.kernel.outputscale.item():.3f}\\n\"\n",
    "      f\"Lengthscale: {model.kernel.base_kernel.lengthscale.detach().cpu().numpy()[0]}\\n\"\n",
    "      f\"Noise variance: {model.likelihood.noise.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhHGFXlALB9m"
   },
   "source": [
    "**Task**: calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JnoGo84NLB9n"
   },
   "outputs": [],
   "source": [
    "x_test = np.random.rand(3000, dim)\n",
    "y_test = rosenbrock(x_test)\n",
    "model.eval()\n",
    "\n",
    "# Turn off some optimization if gpytorch for more accurate computation of the prediction\n",
    "with gpytorch.settings.fast_computations(False, False, False):\n",
    "    y_pr = model.predict(torch.from_numpy(x_test)).mean.numpy()\n",
    "\n",
    "    \n",
    "### Your code goes here ### \n",
    "# Calculate Mean Squared Error using y_pr as a prediction\n",
    "\n",
    "\n",
    "\n",
    "### Your code ends here ###\n",
    "\n",
    "\n",
    "print('\\nMSE: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnG9SfMILB9p"
   },
   "source": [
    "# Covariance functions\n",
    "\n",
    "The most popular covariance function is RBF. However, not all the functions can be modelled using RBF covariance function. For example, approximations of discontinuous functions will suffer from oscillations, approximation of curvy function may suffer from oversmoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l26pvu-kLB9q"
   },
   "outputs": [],
   "source": [
    "def heaviside(x):\n",
    "    return np.asfarray(x > 0)\n",
    "\n",
    "\n",
    "def rastrigin(x):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray - 2D array in [0, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : 1D array of values of Rastrigin function\n",
    "    \"\"\"\n",
    "    scale = 8  # 10.24\n",
    "    x = scale * x - scale / 2\n",
    "    y = 10 * x.shape[1] + (x**2).sum(axis=1) - 10 * np.cos(2 * np.pi * x).sum(axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sp3D5v6LB9s"
   },
   "outputs": [],
   "source": [
    "fig = plot_2d_func(rastrigin, 1, 2, title='Rastrigin function')\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = heaviside(x)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x, y)\n",
    "ax.set_title('Heaviside function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFQzus8GLB9u"
   },
   "source": [
    "#### Example of oscillations\n",
    "As you can see there are oscillations in viscinity of discontinuity because we are trying to approximate\n",
    "discontinuous function using infinitily smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RA8r98wKLB9v"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1150061746)\n",
    "X = torch.rand(50, 1) * 2 - 1\n",
    "y = torch.tensor(heaviside(X.numpy())).float().squeeze()\n",
    "\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)\n",
    "train(model, X, y)\n",
    "\n",
    "utils.plot_model(model)\n",
    "plt.ylim([-0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbkvgV-BLB9w"
   },
   "source": [
    "#### Example of oversmoothing\n",
    "Actually, the GP model only approximates trend of the function.\n",
    "All the curves are treated as noise.\n",
    "The knowledge about this (in fact there is some repeated structure) should be incorporated into the model via kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2Cqg2MuLB9x",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = torch.rand(300, 2)\n",
    "y = torch.tensor(rastrigin(X.numpy()), dtype=torch.float32).squeeze()\n",
    "\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)\n",
    "train(model, X, y)\n",
    "\n",
    "fig = plot_2d_func(lambda x: model.predict(torch.tensor(x).float()).mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pj4lkZECLB9z"
   },
   "source": [
    "### Covariance functions\n",
    "\n",
    "Popular covariance functions: `Exponential`, `Matern32`, `Matern52`, `RatQuad`, `Linear`, `Periodic`. \n",
    "\n",
    "* Exponential:\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\exp \\left (-\\frac{r}{l} \\right), \\quad r = \\|x - x'\\|\n",
    "$$\n",
    "\n",
    "* Matern32\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\left (1 + \\sqrt{3}\\frac{r}{l} \\right )\\exp \\left (-\\sqrt{3}\\frac{r}{l} \\right )\n",
    "$$\n",
    "\n",
    "* Matern52\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\left (1 + \\sqrt{5}\\frac{r}{l} + \\frac{5}{3}\\frac{r^2}{l^2} \\right ) \\exp \\left (-\\sqrt{5}\\frac{r}{l} \\right )\n",
    "$$\n",
    "\n",
    "* RatQuad\n",
    "$$\n",
    "k(x, x') = \\left ( 1 + \\frac{r^2}{2\\alpha l^2}\\right )^{-\\alpha}\n",
    "$$\n",
    "\n",
    "* Linear\n",
    "$$\n",
    "k(x, x') = \\sum_i \\sigma_i^2 x_i x_i'\n",
    "$$\n",
    "\n",
    "* Polynomial\n",
    "$$\n",
    "k(x, x') = \\sigma^2 (x^T x' + c)^d\n",
    "$$\n",
    "\n",
    "* Periodic\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\exp\\left ( -2 \\frac{\\sin^2(\\pi r)}{l^2}\\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I03CS445LB9z"
   },
   "outputs": [],
   "source": [
    "covariance_functions = [kernels.RBFKernel(), kernels.SpectralMixtureKernel(2),\n",
    "                        kernels.MaternKernel(nu=5/2), kernels.LinearKernel(power=1),\n",
    "                        kernels.PolynomialKernel(power=2), kernels.PeriodicKernel(),\n",
    "                       ]\n",
    "figure, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "axes = axes.ravel()\n",
    "for i, k in enumerate(covariance_functions):\n",
    "    plot_kernel(k, ax=axes[i])\n",
    "    axes[i].set_title(str(k).split('(')[0])\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5CC2FgMLB91"
   },
   "source": [
    "## Combination of covariance functions\n",
    "\n",
    "* Sum of covariance function is a valid covariance function:\n",
    "\n",
    "$$\n",
    "k(x, x') = k_1(x, x') + k_2(x, x')\n",
    "$$\n",
    "\n",
    "* Product of covariance functions is a valid covariance funciton:\n",
    "$$\n",
    "k(x, x') = k_1(x, x') k_2(x, x')\n",
    "$$\n",
    "\n",
    "### Combinations of covariance functions in GPytorch\n",
    "\n",
    "In GPytorch to combine covariance functions you can just use operators `+` and `*`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eJCMyodLB91"
   },
   "source": [
    "Let's plot some of the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UArlh43cLB92"
   },
   "outputs": [],
   "source": [
    "covariance_functions = [kernels.LinearKernel(power=1), kernels.PeriodicKernel(),\n",
    "                        kernels.RBFKernel()]\n",
    "operations = {'+': lambda x, y: x + y,\n",
    "              '*': lambda x, y: x * y}\n",
    "\n",
    "figure, axes = plt.subplots(len(operations), len(covariance_functions), figsize=(9, 6))\n",
    "\n",
    "import itertools\n",
    "axes = axes.ravel()\n",
    "count = 0\n",
    "for j, base_kernels in enumerate(itertools.combinations(covariance_functions, 2)):\n",
    "    for k, (op_name, op) in enumerate(operations.items()):\n",
    "        kernel = op(base_kernels[0], base_kernels[1])\n",
    "        plot_kernel(kernel, ax=axes[count])\n",
    "        kernel_names = [\n",
    "            str(base_kernels[i]).split('(')[0] for i in [0, 1]\n",
    "        ]\n",
    "        axes[count].set_title('{} {} {}'.format(kernel_names[0], op_name, kernel_names[1]),\n",
    "                              fontsize=14)\n",
    "        count += 1\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qh5V7LWfLB94"
   },
   "source": [
    "### Additive kernels\n",
    "\n",
    "One of the popular approach to model the function of interest is\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^d f_i(x_i) + \\sum_{i < j} f_{ij}(x_i, x_j) + \\ldots\n",
    "$$\n",
    "\n",
    "**Example**: $\\quad f(x_1, x_2) = f_1(x_1) + f_2(x_2)$  \n",
    "To model it using GP use additive kernel $\\quad k(x, y) = k_1(x_1, y_1) + k_2(x_2, y_2)$.\n",
    "\n",
    "More general - add kernels each depending on subset of inputs\n",
    "$$\n",
    "k(x, y) = k_1(x, y) + \\ldots + k_D(x, y),\n",
    "$$\n",
    "where, for example, $k_1(x, x') = k_1(x_1, x_1'), \\; k_2(x, x') = k_2((x_1, x_3), (x_1', x_3'))$, etc.\n",
    "\n",
    "Here is an example of ${\\rm RBF}(x_1) + {\\rm RBF}(x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVVMcDI3LB94"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create kernel\n",
    "k1 = kernels.RBFKernel(active_dims=[0])\n",
    "k2 = kernels.RBFKernel(active_dims=[1])\n",
    "\n",
    "kernel = k1 + k2\n",
    "\n",
    "# evaluate kernel on grid\n",
    "x = torch.meshgrid(torch.linspace(-3, 3, 50), torch.linspace(-3, 3, 50))\n",
    "x = torch.cat([x[0].reshape(-1, 1), x[1].reshape(-1, 1)], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = kernel(x, torch.tensor([[0., 0.]])).evaluate()\n",
    "\n",
    "# Plot kernel\n",
    "figure = plt.figure()\n",
    "ax = figure.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x[:, 0].reshape(50, 50).numpy(),\n",
    "                x[:, 1].reshape(50, 50).numpy(),\n",
    "                z.reshape(50, 50).numpy(), cmap=cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xploxv5pLB96"
   },
   "source": [
    "### Kernels on arbitrary types of objects\n",
    "\n",
    "Kernels can be defined over all types of data structures: text, images, matrices, graphs, etc. You just need to define similarity between objects.\n",
    "\n",
    "#### Kernels on categorical data\n",
    "\n",
    "* Represent your categorical variable as a by a one-of-k encoding: $\\quad x = (x_1, \\ldots, x_k)$.\n",
    "* Use RBF kernel with `ARD=True`: $\\quad k(x , x') = \\sigma^2 \\prod_{i = 1}^k\\exp{\\left ( -\\dfrac{(x_i - x_i')^2}{\\sigma_i^2} \\right )}$. The lengthscale will now encode whether the rest of the function changes.\n",
    "* Short lengthscales for categorical variables means your model is not sharing any information between data of different categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLwC4OxULB97"
   },
   "source": [
    "## 2 Sampling from GP\n",
    "\n",
    "So, you have defined some complex kernel.\n",
    "You can plot it to see how it looks and guess what kind of functions it can approximate.\n",
    "Another way to do it is to actually generate random functions using this kernel.\n",
    "\n",
    "GP defines distribution over functions, which is defined by its *mean function* $m(x)$ and *covariance function* $k(x, y)$: for any set $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\in \\mathbb{R}^d \\rightarrow$ $\\left (f(\\mathbf{x}_1), \\ldots, f(\\mathbf{x}_N) \\right ) \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{K})$,\n",
    "where $\\mathcal{m} = (m(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N)$, $\\mathbf{K} = \\|k(\\mathbf{x}_i, \\mathbf{x}_j)\\|_{i,j=1}^N$.\n",
    "\n",
    "Sampling procedure:\n",
    "\n",
    "1. Generate set of points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$.\n",
    "2. Calculate mean and covariance matrix $\\mathcal{m} = (m(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N)$, $\\mathbf{K} = \\|k(\\mathbf{x}_i, \\mathbf{x}_j)\\|_{i,j=1}^N$.\n",
    "3. Generate vector from multivariate normal distribution $\\mathcal{N}(\\mathbf{m}, \\mathbf{K})$.\n",
    "\n",
    "Below try to change RBF kernel to some other kernel and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o12biZXsLB97"
   },
   "outputs": [],
   "source": [
    "kernel = kernels.MaternKernel(nu=5/2)\n",
    "\n",
    "X = torch.linspace(0, 5, 500)\n",
    "\n",
    "mu = np.zeros(500)\n",
    "\n",
    "with torch.no_grad():\n",
    "    C = kernel(X).evaluate().numpy()\n",
    "\n",
    "Z = np.random.multivariate_normal(mu, C, 3)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(3):\n",
    "    plt.plot(X.numpy(), Z[i, :])\n",
    "    plt.title(str(kernel).split('(')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXhUGGQ8LB99"
   },
   "source": [
    "### Task\n",
    "\n",
    "Build a GP model that predicts airline passenger counts on international flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tZfG3XGLB9-"
   },
   "outputs": [],
   "source": [
    "data = np.load('airline.npz')\n",
    "\n",
    "X = torch.tensor(data['X'])\n",
    "y = torch.tensor(data['y']).squeeze()\n",
    "\n",
    "train_indices = list(range(70)) + list(range(90, 129))\n",
    "test_indices = range(70, 90)\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(X_train.numpy(), y_train.numpy(), '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DaGCyPRLB-A"
   },
   "source": [
    "You need to obtain something like this\n",
    "\n",
    "![](https://raw.githubusercontent.com/yeahrmek/BayesOpt_tutorial/master/GP/airline_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuXOLDlOLB-B"
   },
   "source": [
    "#### Let's try RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZcqcY78LB-B"
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sacvq1aSLB-C"
   },
   "source": [
    "As you can see below it doesn't work ;("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzCPsSEkLB-C"
   },
   "outputs": [],
   "source": [
    "model = GPRegressor(X_train, y_train, k_rbf).double()\n",
    "train(model, X_train, y_train, n_epochs=500)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "    \n",
    "xlim = [1948, 1964]\n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBd_PNcKLB-E"
   },
   "source": [
    "We will try to model this data set using 3 additive components: trend, seasonality and noise.  \n",
    "So, the kernel should be a sum of 3 kernels:  \n",
    "`kernel = kernel_trend + kernel_seasonality + kernel_noise`\n",
    "\n",
    "#### Let's first try to model trend\n",
    "\n",
    "Trend is almost linear with some small nonlinearity, so you can use sum of linear kernel with some other which gives this small nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ozydctjLB-F"
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27ke5JNKLB-H"
   },
   "outputs": [],
   "source": [
    "model = GPRegressor(X_train, y_train, k_trend).double()\n",
    "train(model, X_train, y_train, n_epochs=1000)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "    \n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LjGdGqPLB-J"
   },
   "source": [
    "#### Let's model periodicity\n",
    "Just periodic kernel will not work (why?).\n",
    "Try to use product of periodic kernel with some other kernel (or maybe 2 other kernels).\n",
    "Note that the amplitude increases with x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gct0Ab57LB-J"
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bED7QClhLB-K"
   },
   "outputs": [],
   "source": [
    "kernel = kernels.AdditiveKernel(k_trend, k_seasonal)\n",
    "\n",
    "model = GPRegressor(X_train, y_train, kernel).double()\n",
    "\n",
    "train(model, X_train, y_train, n_epochs=1000)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "    \n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znAZcCuNLB-M"
   },
   "source": [
    "#### Let's add noise model\n",
    "The dataset is heteroscedastic, i.e. noise variance depends on x: it increases linearly with x.  \n",
    "To model homoscedastic white noise we implement `WhiteNoiseKernel`, but it assumes that noise variance is the same at every x.\n",
    "By what kernel it should be multiplied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlfKHlJkLB-M"
   },
   "outputs": [],
   "source": [
    "from gpytorch.lazy import DiagLazyTensor, ZeroLazyTensor\n",
    "\n",
    "class WhiteNoiseKernel(kernels.Kernel):\n",
    "    def __init__(self, noise=1):\n",
    "        super().__init__()\n",
    "        self.noise = noise\n",
    "    \n",
    "    def forward(self, x1, x2, **params):\n",
    "        if self.training and torch.equal(x1, x2):\n",
    "            return DiagLazyTensor(torch.ones(x1.shape[0]).to(x1) * self.noise)\n",
    "        elif x1.size(-2) == x2.size(-2) and torch.equal(x1, x2):\n",
    "            return DiagLazyTensor(torch.ones(x1.shape[0]).to(x1) * self.noise)\n",
    "        else:\n",
    "            return torch.zeros(x1.shape[0], x2.shape[0]).to(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rANsqln5LB-N"
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4OTPkwyLB-P",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernel = kernels.AdditiveKernel(k_trend, k_seasonal, k_noise)\n",
    "\n",
    "model = GPRegressor(X_train, y_train, kernel).double()\n",
    "\n",
    "train(model, X_train, y_train, n_epochs=1000)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "\n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRFhRG2rLB-6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Xploxv5pLB96"
   ],
   "name": "gp_practice.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
