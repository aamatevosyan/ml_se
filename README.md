
## Contacts

[Telegram chat](https://t.me/ml_se21)

**Lecturers**: [Anna Kuzina](https://akuzina.github.io/); [Evgenii Egorov](https://evgenii-egorov.github.io/)

**Class Teachers and TAs**

| Class Teachers | Contact | Group| TA (contact)|  
|----------------|---------|------|-------|
|Maria Tikhonova|tg: @mashkka_t|БПИ184|Alexandra Kogan (tg: @horror_in_black)|
|Maksim Karpov|tg: @buntar29|БПИ181, БПИ182 |Kirill Bykov (tg: @darkydash), Victor Grishanin (tg: @vgrishanin)|
|Polina Polinuna|tg: @ppolunina|БПИ185|Michail Kim (tg: @kimihailv)|
|Vadim Kokhtev|tg: @despairazure|БПИ183|Daniil Kosakin (tg: @nieto95)|


Use [this form](https://forms.gle/KeGbnntmsPcQXzhX6) to send feedback to the course team anytime 

## Recomended Literature

[PR] Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.\
[Link](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

[ESL] Hastie, T., Hastie, T., Tibshirani, R., & Friedman, J. H. (2001). The elements of statistical learning: Data mining, inference, and prediction. New York: Springer.\
[Link](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)

[FML] Mohri, M., Talwalkar, A., & Rostamizadeh, A. Second Edition, (2018). Foundations of Machine Learning. Cambridge, MA: The MIT Press.\
[Link](https://cs.nyu.edu/~mohri/mlbook/)

## Class materials

#### Lectures

[Lecture Recordings](https://eduhseru-my.sharepoint.com/:f:/g/personal/kroslovtseva_hse_ru/EsszFtVh8qdOuM_S2xhHYtIBdMyX2qaI6QGMwax-2AoTTQ?e=2O0OeF)

| Date | Topic | Lecture materials| Reading|
|------|-------|------------------|--------|
|30 jan|1.Introduction| [Slides](lectures/lecture1_intro.pdf) |[FML] Ch 1; [ESL] Ch 2.1-2 |
|6 feb|2.Gradient Optimization| [Slides](lectures/lecture2_gd.pdf) | [FML] Appx A, B; [Convex Optimization book](https://web.stanford.edu/~boyd/cvxbook/)|
|13 feb|3.Linear Regression| [Slides](lectures/lecture_3.slides.html), [Notebook](lectures/lecture_3.ipynb) |[PR] Ch 3.1; [ESL] Ch 3.1-4;  [FML] Ch 4.4-6|
|20 feb|4.Linear Classification| [Slides (GLM)](lectures/lecture_4_intro.pdf), [Notes (GLM)](lectures/lecture_4_notes_GLM.pdf) ,  [Slides (linclass)](lectures/lecture_4_linclass.pdf)  |[PR] Ch 4.1;  [ESL] Ch 4.1-2, 4.4; [FML] Ch 13|   
|27 feb|5.Logistic Regression and SVM| [Slides](lectures/lecture_5.slides.html) |[ESL] Ch 12.1-3; [FML] Ch 5, 6  |
|6 mar|6.Decision Trees| [Slides](lectures/lecture_6_trees.pdf) | [ESL] Ch 9.2|
|12 mar|7.Bagging, Random Forest| [Slides](lectures/lecture_7.slides.html), [Notebook](lectures/lecture_7.ipynb)|[PR] Ch 3.2 (bias-variance) [ESL] Ch 8  [FML] Ch 7|
|19 mar|8.Gradient boosting| [Slides](lectures/lecture_8.pdf) |  [PR] Ch 14.3 [ESL] Ch 10|
|22 mar - 4 apr| NO LECTURES | --- | --- |
|9 apr|9.Clustering and Anomaly Detection | [Slides](lectures/lecture_9.slides.html), [Notebook](lectures/lecture_9.ipynb) |  |
|16 apr|10.EM and PCA | [Lecture notes](lectures/lecture10_em.pdf) |  |
|23 apr|11.Bayesian Linear Regression| [Slides](lectures/Lecture11_intro_bml.pdf) |  |
|30 apr|12.GP for regression and classification tasks |  |  |
|14 may|13.MLP and DNN for Classification |  |  |
|21 may|14.Deep Generative Models |  |  |
|28 may|15.Summary |  |  |


#### Practicals

| Date | Topic | Materials| Extra Reading/Practice|  
|------|-------|----------|-----------------------|
|25-30 jan|1.Basic toolbox| [Notebook](practicals/Seminar_1/01_HSE_PE_Intro_to_Python_v4.ipynb); [Dataset](https://drive.google.com/drive/folders/1LeZ6JutPcRELcTi198AJe2n0tvgh_AAD?usp=sharing)|[Python Crash Course](practicals/Seminar_1/Additional_notebooks/)|
|1-6 feb|2.EDA and Scikit-learn| [Notebook](practicals/Seminar_2/02_HSE_SE_EDA_v1.ipynb) ||
|8-13 feb|3.Calculus recap and Gradient Descent| [Notebook](practicals/Seminar_3/sem03-gd.ipynb), [pdf](practicals/Seminar_3/sem03-vector-diff.pdf) |[The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi//matrixcookbook.pdf)|
|15-20 feb|4.Linear Regression|[Notebook](practicals/Seminar_4/04_HSE_SE_Linear_regression_v3.ipynb)  ||
|22-27 feb|5.Classification| [Notebook](practicals/Seminar_5/05_HSE_PE_Classification_v2.ipynb) ||
|1-6 mar|6.Texts and Multiclass classification|  [Notebook](practicals/Seminar_6/Seminar_6_intro_to_NLP.ipynb), [Dataset](practicals/Seminar_6/text_lemmatized.zip) ||
|8-13 mar|7.Decision Trees| [Notebook](practicals/Seminar_7/Seminar_07_Decision_trees.ipynb) ||
|15-20 mar|8.Ensembles| [Notebook](practicals/Seminar_8/Seminar_8_ensembles.ipynb)   | |
|5-10 apr|9.Gradient Boosting | [Notebook](practicals/Seminar_9/sem09-gbm.ipynb)  |  |
|12-17 apr|10.Anomaly detection and Clustering | [Notebook](practicals/Seminar_10/sem10_clustering_anomaly_detection_v1.2.ipynb)  |  |
|19-24 apr|11.EM | [Tasks](practicals/Seminar_11/sem11-em.pdf) |  |
|25-30 apr|12.Empirical Bayes and RVM |  [Notebook](practicals/Seminar_12/sem12-bayes_rvm.ipynb)  |  |
|10-15 may|13.GP  |  |  |
|17-22 may|14.MLP |  |  |
|24-29 may|15.Summary |  |  |


## Assignments

We'll be using AnyTask for grading: [course link](https://anytask.org/course/769) 

| Date Published| Task | Deadline | 
|----------------|---------|---------|
| 6 feb  |HW 1: [Notebook](hw/hw_1/task.ipynb), [dataset](hw/hw_1/titanic.csv)| 20 feb|
| 26 feb |HW 2: [Notebook](hw/hw_2/homework-practice-02_v3.ipynb)| 13 mar|
| 14 mar |HW 3: [Notebook](hw/hw_3/HW3_v7.ipynb)| 4 apr|
| 10 apr |HW 4: [Notebook](hw/hw_4/hw4.ipynb), [dataset](hw/hw_4/thyroid_disease.csv)| 1 may|
|   |HW 5: `TBA`| |
|   |HW 6 (Optional): `TBA`| |




## Grading
```Final grade = 0.7*HW + 0.3*Exam```

* `HW` - Average grade for the assignments 1 to 5. 
You can get extra points by solving HW 6, but no more than 10 in total. 
* `Exam` -  Grade for the exam
 
 ---
 
You can skip the exam if your average grade for the first 5 assignemnts is **not smaller** than 6 (`HW >=6`). 
In this case:

```Final grade = HW```
