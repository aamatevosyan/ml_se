{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T16:48:20.566549Z",
     "start_time": "2020-09-26T16:48:19.893995Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this homework we use Boston Dataset from sklearn (based on UCI ML housing dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston() # load dataset\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "columns = data.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. [0.5 points] Create Pandas DataFrame and split the data into train and test sets with ratio 80:20 with random_state=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  target  \n",
       "0       15.3  396.90   4.98    24.0  \n",
       "1       17.8  396.90   9.14    21.6  \n",
       "2       17.8  392.83   4.03    34.7  \n",
       "3       18.7  394.63   2.94    33.4  \n",
       "4       18.7  396.90   5.33    36.2  \n",
       "..       ...     ...    ...     ...  \n",
       "501     21.0  391.99   9.67    22.4  \n",
       "502     21.0  396.90   9.08    20.6  \n",
       "503     21.0  396.90   5.64    23.9  \n",
       "504     21.0  393.45   6.48    22.0  \n",
       "505     21.0  396.90   7.88    11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X, columns=columns)\n",
    "df['target'] = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "old_X_train = X_train\n",
    "old_X_test = X_test\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [1 point] Train models on train data using StatsModels library and apply it to the test set; use $RMSE$ and $R^2$ as the quality measure.\n",
    "\n",
    "* [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
    "* [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with $\\alpha = 0.01$;\n",
    "* [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) with $\\alpha = 0.01$\n",
    "\n",
    "Don't forget to scale the data before training the models with StandardScaler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scale(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    return (sm.add_constant(scaler.fit_transform(X_train)), sm.add_constant(scaler.transform(X_test)))\n",
    "\n",
    "X_train, X_test = custom_scale(old_X_train, old_X_test)\n",
    "\n",
    "alpha = 0.01\n",
    "p_value = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rmse(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"Train RMSE = %.4f\" % rmse(y_train, y_train_pred))\n",
    "    print(\"Test RMSE = %.4f\" % rmse(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 4.3962\n",
      "Test RMSE = 5.7835\n"
     ]
    }
   ],
   "source": [
    "# Linear\n",
    "linear_model = sm.OLS(y_train, X_train)\n",
    "linear_results = linear_model.fit()\n",
    "\n",
    "show_rmse(linear_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 4.4039\n",
      "Test RMSE = 5.8270\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "ridge_model = linear_model.fit_regularized(alpha=alpha, L1_wt=0)\n",
    "ridge_results = sm.regression.linear_model.OLSResults(linear_model, ridge_model.params, linear_model.normalized_cov_params)\n",
    "\n",
    "show_rmse(ridge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE = 4.3968\n",
      "Test RMSE = 5.7962\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "lasso_model = linear_model.fit_regularized(alpha=alpha, L1_wt=1)\n",
    "lasso_results = sm.regression.linear_model.OLSResults(linear_model, lasso_model.params, linear_model.normalized_cov_params)\n",
    "\n",
    "show_rmse(lasso_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [1 point] Explore the values of the parameters of the resulting models and compare the number of zero weights in them. Comment on the significance of the coefficients, overal model significance and other related factors from the results table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The significance level\n",
    "\n",
    "The P-Value as you know provides probability of the hypothesis test,So in a regression model the P-Value for each independent variable tests the Null Hypothesis that there is “No Correlation” between the independent and the dependent variable,this also helps to determine the relationship observed in the sample also exists in the larger data.\n",
    "So if the P-Value is less than the significance level (usually 0.05) then your model fits the data well.\n",
    "\n",
    "The significance level is the probability of rejecting the null hypothesis when it is true.\n",
    "\n",
    "For the models above all coeff-s which have **p-value > 0,05** will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R² score\n",
    "\n",
    "As for the R² metric, it measures the proportion of variability in the target that can be explained using a feature X. Therefore, assuming a linear relationship, if feature X can explain (predict) the target, then the proportion is high and the R² value will be close to 1. If the opposite is true, the R² value is then closer to 0.\n",
    "\n",
    "### Statsmodel\n",
    "\n",
    "For calculations statsmodel.api will be used. As for **R²** score calculation, **Adj. R-squared** will be used, as **R-squared** is biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_repr(x):\n",
    "    if x > 0:\n",
    "        return f\"+ {x} \"\n",
    "    else:\n",
    "        return f\" {x} \"\n",
    "\n",
    "def get_coef_str(a, i):\n",
    "    return f\"{get_str_repr(a)} \\cdot x_{i} \"\n",
    "\n",
    "def get_equation(results):\n",
    "    coeffs = {}\n",
    "    \n",
    "    for i in range(len(results.pvalues)):\n",
    "        if results.pvalues[i] <= p_value:\n",
    "            coeffs[i] = round(results.params[i], 5)\n",
    "\n",
    "    latex = \"\"\n",
    "    for i in coeffs:\n",
    "        if i == 0:\n",
    "            latex += f\" {coeffs[i]} \"\n",
    "        else:\n",
    "            latex += get_coef_str(coeffs[i], i)\n",
    "    \n",
    "    return f\"$y = {latex} $\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2370.9385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-03-10 16:54</td>        <td>BIC:</td>         <td>2426.9583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1171.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>102.2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>9.64e-117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.773</td>            <td>Scale:</td>         <td>20.020</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>     <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>22.6119</td>  <td>0.2226</td>  <td>101.5764</td> <td>0.0000</td> <td>22.1742</td> <td>23.0495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-0.9708</td>  <td>0.2980</td>   <td>-3.2575</td> <td>0.0012</td> <td>-1.5568</td> <td>-0.3849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>1.0571</td>   <td>0.3408</td>   <td>3.1022</td>  <td>0.0021</td> <td>0.3872</td>  <td>1.7271</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>0.0383</td>   <td>0.4428</td>   <td>0.0865</td>  <td>0.9311</td> <td>-0.8324</td> <td>0.9090</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>0.5945</td>   <td>0.2291</td>   <td>2.5946</td>  <td>0.0098</td> <td>0.1440</td>  <td>1.0450</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-1.8551</td>  <td>0.4846</td>   <td>-3.8282</td> <td>0.0002</td> <td>-2.8079</td> <td>-0.9024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>2.5732</td>   <td>0.3175</td>   <td>8.1058</td>  <td>0.0000</td> <td>1.9491</td>  <td>3.1974</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-0.0876</td>  <td>0.4022</td>   <td>-0.2178</td> <td>0.8277</td> <td>-0.8784</td> <td>0.7032</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-2.8809</td>  <td>0.4446</td>   <td>-6.4800</td> <td>0.0000</td> <td>-3.7550</td> <td>-2.0068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>2.1122</td>   <td>0.6069</td>   <td>3.4805</td>  <td>0.0006</td> <td>0.9191</td>  <td>3.3054</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-1.8753</td>  <td>0.6652</td>   <td>-2.8191</td> <td>0.0051</td> <td>-3.1832</td> <td>-0.5675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-2.2928</td>  <td>0.3003</td>   <td>-7.6359</td> <td>0.0000</td> <td>-2.8831</td> <td>-1.7024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>0.7182</td>   <td>0.2613</td>   <td>2.7486</td>  <td>0.0063</td> <td>0.2045</td>  <td>1.2319</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>-3.5925</td>  <td>0.3954</td>   <td>-9.0855</td> <td>0.0000</td> <td>-4.3698</td> <td>-2.8151</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>141.494</td>  <td>Durbin-Watson:</td>    <td>1.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>629.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.470</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.365</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2370.9385\n",
       "Date:               2021-03-10 16:54 BIC:                2426.9583\n",
       "No. Observations:   404              Log-Likelihood:     -1171.5  \n",
       "Df Model:           13               F-statistic:        102.2    \n",
       "Df Residuals:       390              Prob (F-statistic): 9.64e-117\n",
       "R-squared:          0.773            Scale:              20.020   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.6119     0.2226   101.5764   0.0000   22.1742   23.0495\n",
       "x1        -0.9708     0.2980    -3.2575   0.0012   -1.5568   -0.3849\n",
       "x2         1.0571     0.3408     3.1022   0.0021    0.3872    1.7271\n",
       "x3         0.0383     0.4428     0.0865   0.9311   -0.8324    0.9090\n",
       "x4         0.5945     0.2291     2.5946   0.0098    0.1440    1.0450\n",
       "x5        -1.8551     0.4846    -3.8282   0.0002   -2.8079   -0.9024\n",
       "x6         2.5732     0.3175     8.1058   0.0000    1.9491    3.1974\n",
       "x7        -0.0876     0.4022    -0.2178   0.8277   -0.8784    0.7032\n",
       "x8        -2.8809     0.4446    -6.4800   0.0000   -3.7550   -2.0068\n",
       "x9         2.1122     0.6069     3.4805   0.0006    0.9191    3.3054\n",
       "x10       -1.8753     0.6652    -2.8191   0.0051   -3.1832   -0.5675\n",
       "x11       -2.2928     0.3003    -7.6359   0.0000   -2.8831   -1.7024\n",
       "x12        0.7182     0.2613     2.7486   0.0063    0.2045    1.2319\n",
       "x13       -3.5925     0.3954    -9.0855   0.0000   -4.3698   -2.8151\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             141.494       Durbin-Watson:          1.996  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       629.882\n",
       "Skew:                1.470         Prob(JB):               0.000  \n",
       "Kurtosis:            8.365         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$y =  22.61188  -0.97082  \\cdot x_1 + 1.05715  \\cdot x_2 + 0.59451  \\cdot x_4  -1.85515  \\cdot x_5 + 2.57322  \\cdot x_6  -2.88094  \\cdot x_8 + 2.11225  \\cdot x_9  -1.87533  \\cdot x_10  -2.29277  \\cdot x_11 + 0.71818  \\cdot x_12  -3.59245  \\cdot x_13  $\n"
     ]
    }
   ],
   "source": [
    "print(get_equation(linear_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted equation\n",
    "\n",
    "$y =  22.61188  -0.97082  \\cdot x_1 + 1.05715  \\cdot x_2 + 0.59451  \\cdot x_4  -1.85515  \\cdot x_5 + 2.57322  \\cdot x_6  -2.88094  \\cdot x_8 + 2.11225  \\cdot x_9  -1.87533  \\cdot x_10  -2.29277  \\cdot x_11 + 0.71818  \\cdot x_12  -3.59245  \\cdot x_13  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2372.3540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-03-10 16:54</td>        <td>BIC:</td>         <td>2428.3738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1172.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>101.7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>1.90e-116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.772</td>            <td>Scale:</td>         <td>20.091</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>     <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>22.3880</td>  <td>0.2230</td>  <td>100.3946</td> <td>0.0000</td> <td>21.9496</td> <td>22.8264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-0.9389</td>  <td>0.2986</td>   <td>-3.1449</td> <td>0.0018</td> <td>-1.5259</td> <td>-0.3519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>0.9965</td>   <td>0.3414</td>   <td>2.9190</td>  <td>0.0037</td> <td>0.3253</td>  <td>1.6676</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>-0.0576</td>  <td>0.4436</td>   <td>-0.1298</td> <td>0.8968</td> <td>-0.9298</td> <td>0.8146</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>0.6098</td>   <td>0.2295</td>   <td>2.6566</td>  <td>0.0082</td> <td>0.1585</td>  <td>1.0611</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-1.7222</td>  <td>0.4854</td>   <td>-3.5477</td> <td>0.0004</td> <td>-2.6766</td> <td>-0.7678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>2.6120</td>   <td>0.3180</td>   <td>8.2135</td>  <td>0.0000</td> <td>1.9868</td>  <td>3.2372</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-0.1155</td>  <td>0.4029</td>   <td>-0.2867</td> <td>0.7745</td> <td>-0.9078</td> <td>0.6767</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-2.7539</td>  <td>0.4454</td>   <td>-6.1834</td> <td>0.0000</td> <td>-3.6295</td> <td>-1.8783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>1.8348</td>   <td>0.6079</td>   <td>3.0181</td>  <td>0.0027</td> <td>0.6396</td>  <td>3.0301</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-1.6265</td>  <td>0.6664</td>   <td>-2.4407</td> <td>0.0151</td> <td>-2.9366</td> <td>-0.3163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-2.2561</td>  <td>0.3008</td>   <td>-7.5007</td> <td>0.0000</td> <td>-2.8475</td> <td>-1.6648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>0.7188</td>   <td>0.2617</td>   <td>2.7463</td>  <td>0.0063</td> <td>0.2042</td>  <td>1.2334</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>-3.5287</td>  <td>0.3961</td>   <td>-8.9086</td> <td>0.0000</td> <td>-4.3074</td> <td>-2.7499</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>147.660</td>  <td>Durbin-Watson:</td>    <td>1.990</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>694.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.521</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.659</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2372.3540\n",
       "Date:               2021-03-10 16:54 BIC:                2428.3738\n",
       "No. Observations:   404              Log-Likelihood:     -1172.2  \n",
       "Df Model:           13               F-statistic:        101.7    \n",
       "Df Residuals:       390              Prob (F-statistic): 1.90e-116\n",
       "R-squared:          0.772            Scale:              20.091   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.3880     0.2230   100.3946   0.0000   21.9496   22.8264\n",
       "x1        -0.9389     0.2986    -3.1449   0.0018   -1.5259   -0.3519\n",
       "x2         0.9965     0.3414     2.9190   0.0037    0.3253    1.6676\n",
       "x3        -0.0576     0.4436    -0.1298   0.8968   -0.9298    0.8146\n",
       "x4         0.6098     0.2295     2.6566   0.0082    0.1585    1.0611\n",
       "x5        -1.7222     0.4854    -3.5477   0.0004   -2.6766   -0.7678\n",
       "x6         2.6120     0.3180     8.2135   0.0000    1.9868    3.2372\n",
       "x7        -0.1155     0.4029    -0.2867   0.7745   -0.9078    0.6767\n",
       "x8        -2.7539     0.4454    -6.1834   0.0000   -3.6295   -1.8783\n",
       "x9         1.8348     0.6079     3.0181   0.0027    0.6396    3.0301\n",
       "x10       -1.6265     0.6664    -2.4407   0.0151   -2.9366   -0.3163\n",
       "x11       -2.2561     0.3008    -7.5007   0.0000   -2.8475   -1.6648\n",
       "x12        0.7188     0.2617     2.7463   0.0063    0.2042    1.2334\n",
       "x13       -3.5287     0.3961    -8.9086   0.0000   -4.3074   -2.7499\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             147.660       Durbin-Watson:          1.990  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       694.793\n",
       "Skew:                1.521         Prob(JB):               0.000  \n",
       "Kurtosis:            8.659         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$y =  22.388  -0.93892  \\cdot x_1 + 0.99647  \\cdot x_2 + 0.60978  \\cdot x_4  -1.72221  \\cdot x_5 + 2.612  \\cdot x_6  -2.75391  \\cdot x_8 + 1.83481  \\cdot x_9  -1.62648  \\cdot x_10  -2.25612  \\cdot x_11 + 0.71883  \\cdot x_12  -3.52866  \\cdot x_13  $\n"
     ]
    }
   ],
   "source": [
    "print(get_equation(ridge_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted equation\n",
    "\n",
    "$y =  22.388  -0.93892  \\cdot x_1 + 0.99647  \\cdot x_2 + 0.60978  \\cdot x_4  -1.72221  \\cdot x_5 + 2.612  \\cdot x_6  -2.75391  \\cdot x_8 + 1.83481  \\cdot x_9  -1.62648  \\cdot x_10  -2.25612  \\cdot x_11 + 0.71883  \\cdot x_12  -3.52866  \\cdot x_13  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.765</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2371.0439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-03-10 16:54</td>        <td>BIC:</td>         <td>2427.0637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1171.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>13</td>           <td>F-statistic:</td>       <td>102.1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>390</td>       <td>Prob (F-statistic):</td> <td>1.01e-116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.773</td>            <td>Scale:</td>         <td>20.025</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>     <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>22.6019</td>  <td>0.2226</td>  <td>101.5182</td> <td>0.0000</td> <td>22.1642</td> <td>23.0396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-0.9403</td>  <td>0.2981</td>   <td>-3.1547</td> <td>0.0017</td> <td>-1.5263</td> <td>-0.3543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>1.0215</td>   <td>0.3408</td>   <td>2.9972</td>  <td>0.0029</td> <td>0.3514</td>  <td>1.6916</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>0.0000</td>   <td>0.4429</td>   <td>0.0000</td>  <td>1.0000</td> <td>-0.8708</td> <td>0.8708</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>0.5948</td>   <td>0.2292</td>   <td>2.5955</td>  <td>0.0098</td> <td>0.1442</td>  <td>1.0453</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-1.8029</td>  <td>0.4847</td>   <td>-3.7200</td> <td>0.0002</td> <td>-2.7558</td> <td>-0.8500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>2.5852</td>   <td>0.3175</td>   <td>8.1423</td>  <td>0.0000</td> <td>1.9609</td>  <td>3.2094</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-0.0690</td>  <td>0.4023</td>   <td>-0.1715</td> <td>0.8639</td> <td>-0.8599</td> <td>0.7220</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-2.8085</td>  <td>0.4446</td>   <td>-6.3162</td> <td>0.0000</td> <td>-3.6827</td> <td>-1.9343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>1.9567</td>   <td>0.6070</td>   <td>3.2238</td>  <td>0.0014</td> <td>0.7634</td>  <td>3.1501</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-1.7392</td>  <td>0.6653</td>   <td>-2.6141</td> <td>0.0093</td> <td>-3.0472</td> <td>-0.4311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-2.2788</td>  <td>0.3003</td>   <td>-7.5884</td> <td>0.0000</td> <td>-2.8692</td> <td>-1.6884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>0.7056</td>   <td>0.2613</td>   <td>2.7000</td>  <td>0.0072</td> <td>0.1918</td>  <td>1.2193</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>-3.5969</td>  <td>0.3955</td>   <td>-9.0956</td> <td>0.0000</td> <td>-4.3744</td> <td>-2.8194</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>143.585</td>  <td>Durbin-Watson:</td>    <td>1.997</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>649.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.488</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.455</td>   <td>Condition No.:</td>     <td>10</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.765    \n",
       "Dependent Variable: y                AIC:                2371.0439\n",
       "Date:               2021-03-10 16:54 BIC:                2427.0637\n",
       "No. Observations:   404              Log-Likelihood:     -1171.5  \n",
       "Df Model:           13               F-statistic:        102.1    \n",
       "Df Residuals:       390              Prob (F-statistic): 1.01e-116\n",
       "R-squared:          0.773            Scale:              20.025   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.6019     0.2226   101.5182   0.0000   22.1642   23.0396\n",
       "x1        -0.9403     0.2981    -3.1547   0.0017   -1.5263   -0.3543\n",
       "x2         1.0215     0.3408     2.9972   0.0029    0.3514    1.6916\n",
       "x3         0.0000     0.4429     0.0000   1.0000   -0.8708    0.8708\n",
       "x4         0.5948     0.2292     2.5955   0.0098    0.1442    1.0453\n",
       "x5        -1.8029     0.4847    -3.7200   0.0002   -2.7558   -0.8500\n",
       "x6         2.5852     0.3175     8.1423   0.0000    1.9609    3.2094\n",
       "x7        -0.0690     0.4023    -0.1715   0.8639   -0.8599    0.7220\n",
       "x8        -2.8085     0.4446    -6.3162   0.0000   -3.6827   -1.9343\n",
       "x9         1.9567     0.6070     3.2238   0.0014    0.7634    3.1501\n",
       "x10       -1.7392     0.6653    -2.6141   0.0093   -3.0472   -0.4311\n",
       "x11       -2.2788     0.3003    -7.5884   0.0000   -2.8692   -1.6884\n",
       "x12        0.7056     0.2613     2.7000   0.0072    0.1918    1.2193\n",
       "x13       -3.5969     0.3955    -9.0956   0.0000   -4.3744   -2.8194\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             143.585       Durbin-Watson:          1.997  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       649.888\n",
       "Skew:                1.488         Prob(JB):               0.000  \n",
       "Kurtosis:            8.455         Condition No.:          10     \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$y =  22.60188  -0.94032  \\cdot x_1 + 1.0215  \\cdot x_2 + 0.59479  \\cdot x_4  -1.80292  \\cdot x_5 + 2.58517  \\cdot x_6  -2.80847  \\cdot x_8 + 1.95673  \\cdot x_9  -1.73918  \\cdot x_10  -2.27881  \\cdot x_11 + 0.70557  \\cdot x_12  -3.59692  \\cdot x_13  $\n"
     ]
    }
   ],
   "source": [
    "print(get_equation(lasso_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted equation\n",
    "\n",
    "$y =  22.60188  -0.94032  \\cdot x_1 + 1.0215  \\cdot x_2 + 0.59479  \\cdot x_4  -1.80292  \\cdot x_5 + 2.58517  \\cdot x_6  -2.80847  \\cdot x_8 + 1.95673  \\cdot x_9  -1.73918  \\cdot x_10  -2.27881  \\cdot x_11 + 0.70557  \\cdot x_12  -3.59692  \\cdot x_13  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conlusion\n",
    "It can be seen that in all three models, share common characteristics. As for significant coeffs, $x_3$ and $x_7$ were dropped. As for $R^2$ score, they have the same Adj. R-squared:\t0.765. Therefore, more than 76% of variability in the target that can be explained using a feature X. This is good, but definitely not the best we can to accurately predict the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [1 point] Implement one of the elimination algorithms that were described in the Seminar_4 (Elimination by P-value, Forward elimination, Backward elimination), make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>               <td>OLS</td>         <td>Adj. R-squared:</td>     <td>0.767</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>               <td>AIC:</td>         <td>2366.9962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-03-10 16:54</td>        <td>BIC:</td>         <td>2415.0132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>         <td>404</td>         <td>Log-Likelihood:</td>    <td>-1171.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>11</td>           <td>F-statistic:</td>       <td>121.3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>           <td>392</td>       <td>Prob (F-statistic):</td> <td>8.15e-119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "      <td>R-squared:</td>            <td>0.773</td>            <td>Scale:</td>         <td>19.921</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>     <th>Coef.</th>  <th>Std.Err.</th>     <th>t</th>     <th>P>|t|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>22.6119</td>  <td>0.2221</td>  <td>101.8292</td> <td>0.0000</td> <td>22.1753</td> <td>23.0485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-0.9694</td>  <td>0.2969</td>   <td>-3.2652</td> <td>0.0012</td> <td>-1.5531</td> <td>-0.3857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>1.0657</td>   <td>0.3349</td>   <td>3.1817</td>  <td>0.0016</td> <td>0.4072</td>  <td>1.7242</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>0.5950</td>   <td>0.2276</td>   <td>2.6145</td>  <td>0.0093</td> <td>0.1476</td>  <td>1.0425</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>-1.8752</td>  <td>0.4455</td>   <td>-4.2092</td> <td>0.0000</td> <td>-2.7511</td> <td>-0.9993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>2.5571</td>   <td>0.3092</td>   <td>8.2693</td>  <td>0.0000</td> <td>1.9492</td>  <td>3.1651</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>-2.8621</td>  <td>0.4144</td>   <td>-6.9062</td> <td>0.0000</td> <td>-3.6768</td> <td>-2.0473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>2.1039</td>   <td>0.5729</td>   <td>3.6725</td>  <td>0.0003</td> <td>0.9776</td>  <td>3.2302</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-1.8509</td>  <td>0.5893</td>   <td>-3.1410</td> <td>0.0018</td> <td>-3.0094</td> <td>-0.6924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-2.2922</td>  <td>0.2953</td>   <td>-7.7616</td> <td>0.0000</td> <td>-2.8729</td> <td>-1.7116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>0.7116</td>   <td>0.2591</td>   <td>2.7460</td>  <td>0.0063</td> <td>0.2021</td>  <td>1.2211</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>-3.6205</td>  <td>0.3676</td>   <td>-9.8482</td> <td>0.0000</td> <td>-4.3433</td> <td>-2.8977</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td>Omnibus:</td>    <td>140.146</td>  <td>Durbin-Watson:</td>    <td>1.999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Prob(Omnibus):</td>  <td>0.000</td>  <td>Jarque-Bera (JB):</td> <td>614.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Skew:</td>      <td>1.459</td>      <td>Prob(JB):</td>      <td>0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Kurtosis:</td>    <td>8.292</td>   <td>Condition No.:</td>      <td>8</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                 Results: Ordinary least squares\n",
       "==================================================================\n",
       "Model:              OLS              Adj. R-squared:     0.767    \n",
       "Dependent Variable: y                AIC:                2366.9962\n",
       "Date:               2021-03-10 16:54 BIC:                2415.0132\n",
       "No. Observations:   404              Log-Likelihood:     -1171.5  \n",
       "Df Model:           11               F-statistic:        121.3    \n",
       "Df Residuals:       392              Prob (F-statistic): 8.15e-119\n",
       "R-squared:          0.773            Scale:              19.921   \n",
       "--------------------------------------------------------------------\n",
       "           Coef.    Std.Err.      t       P>|t|     [0.025    0.975]\n",
       "--------------------------------------------------------------------\n",
       "const     22.6119     0.2221   101.8292   0.0000   22.1753   23.0485\n",
       "x1        -0.9694     0.2969    -3.2652   0.0012   -1.5531   -0.3857\n",
       "x2         1.0657     0.3349     3.1817   0.0016    0.4072    1.7242\n",
       "x3         0.5950     0.2276     2.6145   0.0093    0.1476    1.0425\n",
       "x4        -1.8752     0.4455    -4.2092   0.0000   -2.7511   -0.9993\n",
       "x5         2.5571     0.3092     8.2693   0.0000    1.9492    3.1651\n",
       "x6        -2.8621     0.4144    -6.9062   0.0000   -3.6768   -2.0473\n",
       "x7         2.1039     0.5729     3.6725   0.0003    0.9776    3.2302\n",
       "x8        -1.8509     0.5893    -3.1410   0.0018   -3.0094   -0.6924\n",
       "x9        -2.2922     0.2953    -7.7616   0.0000   -2.8729   -1.7116\n",
       "x10        0.7116     0.2591     2.7460   0.0063    0.2021    1.2211\n",
       "x11       -3.6205     0.3676    -9.8482   0.0000   -4.3433   -2.8977\n",
       "------------------------------------------------------------------\n",
       "Omnibus:             140.146       Durbin-Watson:          1.999  \n",
       "Prob(Omnibus):       0.000         Jarque-Bera (JB):       614.839\n",
       "Skew:                1.459         Prob(JB):               0.000  \n",
       "Kurtosis:            8.292         Condition No.:          8      \n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eliminate_by_p_value(y, X, alpha):\n",
    "    y, X = y.copy(), X.copy(), \n",
    "    \n",
    "    while True:\n",
    "        results = sm.OLS(y, X).fit()\n",
    "        p_values = results.pvalues\n",
    "\n",
    "        if len(p_values[p_values > alpha]) == 0:\n",
    "            break;\n",
    "            \n",
    "        index = np.argmax(p_values)\n",
    "        X = np.delete(X, index, axis=1)\n",
    "        \n",
    "    return results\n",
    "    \n",
    "elimination_results = eliminate_by_p_value(y_train, X_train, p_value)\n",
    "elimination_results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. [1 point] Find the best (in terms of RMSE) $\\alpha$ for Ridge regression using cross-validation with 5 folds. You must select values from range $[10^{-4}, 10^{3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge_model = linear_model.fit_regularized(alpha=alpha, L1_wt=0)\n",
    "# ridge_results = sm.regression.linear_model.OLSResults(linear_model, ridge_model.params, linear_model.normalized_cov_params)\n",
    "\n",
    "class SMRidgeWrapper(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" A universal sklearn-style wrapper for statsmodels regressors \"\"\"\n",
    "    def __init__(self, model_class):\n",
    "        self.model_class = model_class\n",
    "    def fit(self, X, y):\n",
    "        self.linear_model_ = self.model_class(y, X)\n",
    "        self.model_ = self.linear_model_.fit_regularized(alpha=alpha, L1_wt=0)\n",
    "        self.results_ = sm.regression.linear_model.OLSResults(linear_model, ridge_model.params, linear_model.normalized_cov_params)\n",
    "        return self.results_\n",
    "    def predict(self, X):\n",
    "        return self.results_.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores:\n",
      "\t 14.6024\n",
      "\t19.1293\n",
      "\t18.9876\n",
      "\t30.2033\n",
      "\t13.9822\n",
      "Mean CV MSE = 19.3809\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(SMRidgeWrapper(sm.OLS), X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation scores:\\n\\t\", \"\\n\\t\".join(\"%.4f\" % -x for x in cv_scores))\n",
    "print(\"Mean CV MSE = %.4f\" % np.mean(-cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha = 10.1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CV score')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = np.linspace(0.0001, 1000, 100)\n",
    "searcher = GridSearchCV(Ridge(), [{\"alpha\": alphas}], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = searcher.best_params_[\"alpha\"]\n",
    "print(\"Best alpha = %.4f\" % best_alpha)\n",
    "\n",
    "plt.plot(alphas, -searcher.cv_results_[\"mean_test_score\"])\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"CV score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### 6. [3.5 points] Implement a linear regression model for the MSE loss function, trained by gradient descent.\n",
    "\n",
    "All calculations must be vectorized, and python loops can only be used for gradient descent iterations. As a stop criterion, you must use (simultaneously):\n",
    "\n",
    "* checking for the Euclidean norm of the weight difference on two adjacent iterations (for example, less than some small number of the order of $10^{-6}$, set by the `tolerance` parameter);\n",
    "* reaching the maximum number of iterations (for example, 10000, set by the `max_iter` parameter).\n",
    "\n",
    "You need to implement:\n",
    "\n",
    "* Full gradient descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} Q(w_{k}).\n",
    "$$\n",
    "\n",
    "* Stochastic Gradient Descent:\n",
    "\n",
    "$$\n",
    "w_{k + 1} = w_{k} - \\eta_{k} \\nabla_{w} q_{i_{k}}(w_{k}).\n",
    "$$\n",
    "\n",
    "$\\nabla_{w} q_{i_{k}}(w_{k}) \\, $ is the estimate of the gradient over the butch of objects selected randomly.\n",
    "\n",
    "* Momentum method:\n",
    "\n",
    "$$\n",
    "h_0 = 0, \\\\\n",
    "h_{k + 1} = \\alpha h_{k} + \\eta_k \\nabla_{w} Q(w_{k}), \\\\\n",
    "w_{k + 1} = w_{k} - h_{k + 1}.\n",
    "$$\n",
    "\n",
    "\n",
    "To make sure that the optimization process really converges, we will use the `loss_history` class attribute. After calling the `fit` method, it should contain the values of the loss function for all iterations, starting from the first one (before the first step on the anti-gradient).\n",
    "\n",
    "You need to initialize the weights with a zero or random (from a normal distribution) vector. The following is a template class that needs to contain the code implementing all variations of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinReg(BaseEstimator):\n",
    "    def __init__(self, delta=1.0, gd_type='Momentum', \n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, eta=1e-2, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        gd_type: 'GradientDescent', 'StochasticDescent', 'Momentum'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        eta: learning rate\n",
    "        alpha: momentum coefficient\n",
    "        \"\"\"\n",
    "        self.delta = delta\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.eta = eta\n",
    "        self.loss_history = None # list of loss function values at each training iteration\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        self.loss_history = []\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise Exception('Not trained yet')\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d) (l can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: np.array of shape (d)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (l, d)\n",
    "        y: np.array of shape (l)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. [1 points] Train and validate \"hand-written\" models on the same data, and compare the quality with the Sklearn or StatsModels methods. Investigate the effect of the `max_iter` and `alpha` parameters on the optimization process. Is it consistent with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. [1 points] Plot graphs (on the same picture) of the dependence of the loss function value on the iteration number for Full GD, SGD and Momentum. Draw conclusions about the rate of convergence of various modifications of gradient descent.\n",
    "\n",
    "Don't forget about what *beautiful* graphics should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
