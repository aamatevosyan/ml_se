{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d54c62",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 5\n",
    "\n",
    "**Warning 1**: You have 3 weeks for this assignemnt.  **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a1b55",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "#### PCA, t-SNE – 1.25 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.25 points)\n",
    "* [Task 3](#task3) (0.25 points)\n",
    "* [Task 4](#task4) (0.25 points)\n",
    "\n",
    "#### Clustering – 4.25 points\n",
    "* [Task 5](#task5) (1.5 points)\n",
    "* [Task 6](#task6) (1.25 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (1 point)\n",
    "\n",
    "#### EM-algorithm – 4.5 points\n",
    "\n",
    "* [Task 9](#task9) (1 point)\n",
    "* [Task 10](#task10) (1 point)\n",
    "* [Task 11](#task11) (1.25 points)\n",
    "* [Task 12](#task12) (1.25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4dd7a7",
   "metadata": {},
   "source": [
    "Download the file `data_Mar_64.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed27213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/AKuzina/ml_se/main/hw/hw_5/data_Mar_64.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4310d647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acer Campestre</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acer Campestre</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acer Campestre</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acer Campestre</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acer Campestre</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6   \\\n",
       "0  Acer Campestre  0.003906  0.003906  0.027344  0.033203  0.007812  0.017578   \n",
       "1  Acer Campestre  0.005859  0.013672  0.027344  0.025391  0.013672  0.029297   \n",
       "2  Acer Campestre  0.011719  0.001953  0.027344  0.044922  0.017578  0.042969   \n",
       "3  Acer Campestre  0.013672  0.011719  0.037109  0.017578  0.011719  0.087891   \n",
       "4  Acer Campestre  0.007812  0.009766  0.027344  0.025391  0.001953  0.005859   \n",
       "\n",
       "         7         8         9   ...        55        56        57        58  \\\n",
       "0  0.023438  0.005859  0.000000  ...  0.011719  0.000000  0.005859  0.035156   \n",
       "1  0.019531  0.000000  0.001953  ...  0.017578  0.000000  0.021484  0.017578   \n",
       "2  0.023438  0.000000  0.003906  ...  0.035156  0.000000  0.015625  0.021484   \n",
       "3  0.023438  0.000000  0.000000  ...  0.015625  0.001953  0.021484  0.029297   \n",
       "4  0.015625  0.000000  0.005859  ...  0.023438  0.001953  0.021484  0.048828   \n",
       "\n",
       "         59        60        61        62        63   64  \n",
       "0  0.027344  0.033203  0.001953  0.000000  0.017578  0.0  \n",
       "1  0.046875  0.005859  0.003906  0.003906  0.046875  0.0  \n",
       "2  0.056641  0.009766  0.003906  0.000000  0.015625  0.0  \n",
       "3  0.033203  0.003906  0.000000  0.001953  0.027344  0.0  \n",
       "4  0.056641  0.019531  0.000000  0.000000  0.013672  0.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data_Mar_64.txt', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf1b03",
   "metadata": {},
   "source": [
    "This [dataset](https://archive.ics.uci.edu/ml/datasets/One-hundred+plant+species+leaves+data+set) consists of work carried out by James Cope, Charles Mallah, and James Orwell, Kingston University London. The Leaves were collected in the Royal Botanic Gardens, Kew, UK. \n",
    "\n",
    "For Each feature, a 64 element vector is given per sample of leaf. One file for each 64-element feature vectors. **Each row begins with the class label**. Here is the plant leaf **classification task**. The remaining 64 elements is the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31717d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 65)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sixteen samples of leaf each of one-hundred plant species\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe6a8c",
   "metadata": {},
   "source": [
    "The first column is the target, put it in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, y_name = np.array(data.iloc[:, 1:]), data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0aa410",
   "metadata": {},
   "source": [
    "**Task 1. <a id=\"task1\"></a> (0.5 points)** Let's do the following pipeline (detailed instructions will be in next cells)\n",
    "\n",
    "- Encode your textual target.\n",
    "- Split your data into train and test. Train a simple classification model without any improvements and calculate metrics.\n",
    "- Then let's look at the low dimensional representations of the features and look at the classes there. We will use linear method PCA and non-linear t-SNE (t-distributed stochastic neighbor embedding). In this task we learn how to visualize data at the low dimensional space and check whether the obtained points are separable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf1d7d",
   "metadata": {},
   "source": [
    "The target variable takes a text value. Use the `LabelEncoder` from `sklearn` to encode the text variable `y_name` and save the resulting values to the variable `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ad55e",
   "metadata": {},
   "source": [
    "Split your data into **train** and **test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be562a7",
   "metadata": {},
   "source": [
    "Train a simple classifier on your data to predict target. Calculate accuracy, F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ec03f",
   "metadata": {},
   "source": [
    "Let's try Principal Component Analysis. Use the `PCA` method from `sklearn.decomposiion` to reduce the dimension of the feature space to two. Fix `random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77590cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff83e9ca",
   "metadata": {},
   "source": [
    "Select objects that match values from 0 to 14 of the target variable `y`. Draw the selected objects in a two-dimensional feature space using the `scatter` method from `matplotlib.pyplot`. To display objects of different classes in different colors, pass `c = y[y<15]` to the `scatter` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be445a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954db71",
   "metadata": {},
   "source": [
    "Do the same procedure as in two previous cells, but now for the `TSNE` method from `sklearn.manifold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee130c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c787e",
   "metadata": {},
   "source": [
    "**Task 2. <a id=\"task2\"></a> (0.25 points)** Specify the coordinates of the object with index 0 (`X[0]`) after applying the TSNE method. Round the numbers to hundredths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c73a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "cords_1_tsne = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9b5d3",
   "metadata": {},
   "source": [
    "**Task 3. <a id=\"task3\"></a> (0.25 points)** Specify the coordinates of the object with index 0 (`X[0]`) after applying the PCA method. Round the numbers to hundredths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86badc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "cords_1_pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b6ffaa",
   "metadata": {},
   "source": [
    "**Task 4. <a id=\"task4\"></a> (0.25 points)** What conclusions can be drawn from the obtained images? Choose the right one(s).\n",
    "\n",
    "1) Using the principal components method, it was possible to visualize objects on a plane and objects of different classes are visually separable\n",
    "\n",
    "2) Using the TSNE method, it was possible to visualize objects on a plane and objects of different classes are visually separable\n",
    "\n",
    "3) Using the TSNE and PCA methods, it was possible to visualize objects on a plane and objects of different classes are visually separable\n",
    "\n",
    "4) Using the TSNE and PCA methods, it was possible to visualize objects on a plane and objects of different classes are not visually separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c403e5d",
   "metadata": {},
   "source": [
    "##your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a9a99",
   "metadata": {},
   "source": [
    "## K_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb333d",
   "metadata": {},
   "source": [
    "**Task 5. <a id=\"task5\"></a> (1.5 points)** Implement the MyKMeans class.\n",
    "\n",
    "The class must match the template shown below. Please, add code where needed. Some guidelines are the following:\n",
    "\n",
    "The class constructor is passed to:\n",
    "- n_clusters - the number of clusters that the data will be split into\n",
    "\n",
    "- n_iters - the maximum number of iterations that can be done in this algorithm\n",
    "\n",
    "Realize `update_centers` and `update_labels` methods.\n",
    "\n",
    "\n",
    "In the `fit` method:\n",
    "\n",
    "- Write sequential call of `self_centers` and `self_labels`.\n",
    "\n",
    "then in the loop by the number of iterations you need to implement:\n",
    "- calculate the nearest cluster center for each object\n",
    "- recalculate the center of each cluster (the average of each of the coordinates of all objects assigned to this cluster)\n",
    "put the calculated new cluster centers in the `new_centers` variable\n",
    "\n",
    "In the `predict` method:\n",
    "\n",
    "the nearest cluster centers for `X` objects are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def plot_clust(X, centers, lables, ax): \n",
    "    ax.scatter(X[:,0], X[:,1], c=lables)\n",
    "    ax.scatter(centers[:,0], centers[:,1], marker='>', color='red')\n",
    "\n",
    "\n",
    "class MyKMeans():\n",
    "    def __init__(self, n_clusters=3, n_iters=100, seed=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.labels = None \n",
    "        self.centers = None \n",
    "        self.n_iters = n_iters\n",
    "        self.seed = 0 if seed is None else seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def update_centers(self, X):\n",
    "        ## your code here\n",
    "        return centers \n",
    "    \n",
    "    def update_lables(self, X):\n",
    "        ## your code here\n",
    "        return labels \n",
    "\n",
    "    def fit(self, X):\n",
    "        self.centers = ## your code here\n",
    "        \n",
    "        self.labels = ## your code here\n",
    "\n",
    "        for it in range(self.n_iters):\n",
    "            new_labels = self.update_lables(X)\n",
    "            self.labels = new_labels\n",
    "\n",
    "            new_centers = self.update_centers(X)\n",
    "            if np.allclose(self.centers.flatten(), new_centers.flatten(), atol=1e-1):\n",
    "                self.centers = new_centers\n",
    "                self.labels = new_labels\n",
    "                print('Converge by tolerance centers')\n",
    "\n",
    "                fig, ax = plt.subplots(1,1)\n",
    "                plot_clust(X, new_centers, new_labels, ax)\n",
    "                return 0\n",
    "      \n",
    "            self.centers = new_centers\n",
    "\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            plot_clust(X, new_centers, new_labels, ax)\n",
    "            plt.pause(0.3);\n",
    "            clear_output(wait=True);\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        labels = ## your code here\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a52886",
   "metadata": {},
   "source": [
    "Generating data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bca4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "n_samples = 1000\n",
    "\n",
    "noisy_blobs = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 0.5, 0.5],\n",
    "                             random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e29920",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = noisy_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd116403",
   "metadata": {},
   "source": [
    "**Task 6. <a id=\"task6\"></a> (1.25 points)** \n",
    "\n",
    "5.1 Cluster noisy_blobs objects with `MyKMeans`, use the hyperparameters `n_clusters=3`, `n_iters=100`. Specify the response for the object with index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1000ea",
   "metadata": {},
   "source": [
    "5.2 Cluster noisy_blobs objects, use the hyperparameters `n_clusters=3`, `n_iters = 5`. Specify the response for the object with index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cf977",
   "metadata": {},
   "source": [
    "5.3 Calculate how many objects changed the label of the predicted cluster when changing the hyperparameter n_iters from 5 to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "num_of_changed = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff25332",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c0032",
   "metadata": {},
   "source": [
    "**Task 7. <a id=\"task7\"></a> (0.5 points)** Cluster noisy_blobs objects using DBSCAN. Use the DBSCAN implementation from sklearn. Fix the `eps=0.5` hyperparameter. Specify the response for the object with index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c6004",
   "metadata": {},
   "source": [
    "**Task 8. <a id=\"task8\"></a> (1 point)** 8.1 Specify the resulting number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df95f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "num_of_clusters = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f6868",
   "metadata": {},
   "source": [
    "8.2 How many objects were counted as outliers (marked -1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6db8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "num_of_outliers = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21240471",
   "metadata": {},
   "source": [
    "## Implementing EM for GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f798aa",
   "metadata": {},
   "source": [
    "**Multivariate Gaussian mixture model**\n",
    "\n",
    "A Bayesian Gaussian mixture model is commonly extended to fit a vector of unknown parameters (denoted in bold), or multivariate normal distributions.  In a multivariate distribution (i.e. one modelling a vector $\\boldsymbol{x}$ with $N$ random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a Gaussian mixture model prior distribution on the vector of estimates given by\n",
    "\n",
    "$p(\\boldsymbol{\\theta}) = \\sum_{i=1}^K\\phi_i \\mathcal{N}(\\boldsymbol{\\mu_i,\\Sigma_i})$\n",
    "\n",
    "where the $i^{th}$ vector component is characterized by normal distributions with weights $\\phi_i$, means $\\boldsymbol{\\mu_i}$ and covariance matrices $\\boldsymbol{\\Sigma_i}$. To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution $p(\\boldsymbol{x | \\theta})$ of the data $\\boldsymbol{x}$ conditioned on the parameters $\\boldsymbol{\\theta}$ to be estimated.  With this formulation, the posterior distribution $p(\\boldsymbol{\\theta | x})$ is *also* a Gaussian mixture model of the form\n",
    "\n",
    "$p(\\boldsymbol{\\theta | x}) = \\sum_{i=1}^K\\tilde{\\phi_i} \\mathcal{N}(\\boldsymbol{\\tilde{\\mu_i},\\tilde{\\Sigma_i}})$\n",
    "\n",
    "with new parameters $\\tilde{\\phi_i}, \\boldsymbol{\\tilde{\\mu_i}}$ and $\\boldsymbol{\\tilde{\\Sigma_i}}$ that are updated using the Expectation-maximization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9f86c",
   "metadata": {},
   "source": [
    "### Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f64936",
   "metadata": {},
   "source": [
    "Remember, that EM algorithm is a coordinate descent optimization of variational lower bound $\\mathcal{L}(\\theta, q) = \\int q(T) \\log\\frac{p(X, T|\\theta)}{q(T)}dT\\to \\max$.\n",
    "\n",
    "<b>E-step</b>:<br>\n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{q} \\Leftrightarrow \\mathcal{KL} [q(T) \\,\\|\\, p(T|X, \\theta)] \\to \\min \\limits_{q\\in Q} \\Rightarrow q(T) = p(T|X, \\theta)$<br>\n",
    "<b>M-step</b>:<br> \n",
    "$\\mathcal{L}(\\theta, q) \\to \\max\\limits_{\\theta} \\Leftrightarrow \\mathbb{E}_{q(T)}\\log p(X,T | \\theta) \\to \\max\\limits_{\\theta}$\n",
    "\n",
    "For GMM, $\\theta$ is a set of parameters that consists of mean vectors $\\mu_c$, covariance matrices $\\Sigma_c$ and priors $\\pi_c$ for each component.\n",
    "\n",
    "Latent variables $T$ are indices of components to which each data point is assigned, i.e. $t_i$  is the cluster index for object $x_i$.\n",
    "\n",
    "The joint distribution can be written as follows: $\\log p(T, X \\mid \\theta) =  \\sum\\limits_{i=1}^N \\log p(t_i, x_i \\mid \\theta) = \\sum\\limits_{i=1}^N \\sum\\limits_{c=1}^C q(t_i = c) \\log \\left (\\pi_c \\, f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)\\right)$,\n",
    "where $f_{\\!\\mathcal{N}}(x \\mid \\mu_c, \\Sigma_c) = \\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma_c|}}\n",
    "\\exp\\left(-\\frac{1}{2}({x}-{\\mu_c})^T{\\boldsymbol\\Sigma_c}^{-1}({x}-{\\mu_c})\n",
    "\\right)$ is the probability density function (pdf) of the normal distribution $\\mathcal{N}(x_i \\mid \\mu_c, \\Sigma_c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79560ee6",
   "metadata": {},
   "source": [
    "In this part, we will derive and implement formulas for Gaussian Mixture Model — one of the most commonly used methods for performing soft clustering of the data. \n",
    "\n",
    "For debugging, we will use samples from a Gaussian mixture model with unknown mean, variance, and priors. We also added initial values of parameters for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.load('samples.npz')\n",
    "X = samples['data']\n",
    "pi0 = samples['pi0']\n",
    "mu0 = samples['mu0']\n",
    "sigma0 = samples['sigma0']\n",
    "plt.scatter(X[:, 0], X[:, 1], c='grey', s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import slogdet, det, solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94558316",
   "metadata": {},
   "source": [
    "### E-step\n",
    "In this step we need to estimate the posterior distribution over the latent variables with fixed values of parameters: $q_i(t_i) = p(t_i \\mid x_i, \\theta)$. We assume that $t_i$ equals to the cluster index of the true component of the $x_i$ object. To do so we need to compute $\\gamma_{ic} = p(t_i = c \\mid x_i, \\theta)$. Note that $\\sum\\limits_{c=1}^C\\gamma_{ic}=1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312108ea",
   "metadata": {},
   "source": [
    "<b>Important trick 1:</b> It is important to avoid numerical errors. At some point you will have to compute the formula of the following form: $\\frac{e^{y_i}}{\\sum_j e^{y_j}}$, which is called _softmax_. When you compute exponents of large numbers, some numbers may become infinity. You can avoid this by dividing numerator and denominator by $e^{\\max(y)}$: $\\frac{e^{y_i-\\max(y)}}{\\sum_j e^{y_j - \\max(y)}}$. After this transformation maximum value in the denominator will be equal to one. All other terms will contribute smaller values. So, to compute desired formula you first subtract maximum value from each component in vector $\\mathbf{y}$ and then compute everything else as before.\n",
    "\n",
    "<b>Important trick 2:</b> You will probably need to compute formula of the form $A^{-1}x$ at some point. You would normally inverse $A$ and then multiply it by $x$. A bit faster and more numerically accurate way to do this is to directly solve equation $Ay = x$ by using a special function. Its solution is $y=A^{-1}x$, but the equation $Ay = x$ can be solved by methods which do not explicitely invert the matrix. You can use ```np.linalg.solve``` for this.\n",
    "\n",
    "<b>Other usefull functions: </b> <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.slogdet.html\">```slogdet```</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.det.html#numpy.linalg.det\">```det```</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eefd1c",
   "metadata": {},
   "source": [
    "<b>Task 9. <a id=\"task9\"></a> (1 point)</b> Implement E-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80053b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Performs E-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    pi: (C), mixture component weights \n",
    "    mu: (C x d), mixture component means\n",
    "    sigma: (C x d x d), mixture component covariance matrices\n",
    "    \n",
    "    Returns:\n",
    "    gamma: (N x C), probabilities of clusters for objects\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = pi.shape[0] # number of clusters\n",
    "    d = mu.shape[1] # dimension of each object\n",
    "    gamma = np.zeros((N, C)) # distribution q(T)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521192ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(C, pi0, mu0, sigma0)\n",
    "print(np.sum(gamma, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a501d2",
   "metadata": {},
   "source": [
    "### M-step\n",
    "\n",
    "In M-step we need to maximize $\\mathbb{E}_{q(T)}\\log p(X,T | \\theta)$ with respect to $\\theta$. In our model this means that we need to find optimal values of $\\pi$, $\\mu$, $\\Sigma$. To do so, you need to compute the derivatives and \n",
    "set them to zero. You should start by deriving formulas for $\\mu$ as it is the easiest part. Then move on to $\\Sigma$. Here it is crucial to optimize function w.r.t. to $\\Lambda = \\Sigma^{-1}$ and then inverse obtained result. Finaly, to compute $\\pi$, you will need <a href=\"https://www3.nd.edu/~jstiver/FIN360/Constrained%20Optimization.pdf\">Lagrange Multipliers technique</a> to satisfy constraint $\\sum\\limits_{c=1}^{C}\\pi_c = 1$.\n",
    "\n",
    "<br>\n",
    "<b>Important note:</b> You will need to compute derivatives of scalars with respect to matrices. To refresh this technique from previous courses, see <a href=\"https://en.wikipedia.org/wiki/Matrix_calculus\"> wiki article</a> about it . Main formulas of matrix derivatives can be found in <a href=\"http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\">Chapter 2 of The Matrix Cookbook</a>. For example, there you may find that $\\frac{\\partial}{\\partial A}\\log |A| = A^{-T}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f04e3",
   "metadata": {},
   "source": [
    "<b>Task 10. <a id=\"task10\"></a> (1 point)</b> Implement M-step for GMM using template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, gamma):\n",
    "    \"\"\"\n",
    "    Performs M-step on GMM model\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    \n",
    "    Returns:\n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return pi, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c26e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(X, pi0, mu0, sigma0)\n",
    "pi, mu, sigma = M_step(X, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840c91f",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a365b7",
   "metadata": {},
   "source": [
    "Finally, we need some function to track convergence. We will use variational lower bound $\\mathcal{L}$ for this purpose. We will stop our EM iterations when $\\mathcal{L}$ will saturate. Usually, you will need only about 10-20 iterations to converge. It is also useful to check that this function never decreases during training. If it does, you have a bug in your code.\n",
    "\n",
    "<b>Task 11. <a id=\"task11\"></a> (1.25 points)</b> Implement a function that will compute $\\mathcal{L}$ using template below.\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{N} \\sum_{c=1}^{C} q(t_i =c) (\\log \\pi_c + \\log f_{\\!\\mathcal{N}}(x_i \\mid \\mu_c, \\Sigma_c)) - \\sum_{i=1}^{N} \\sum_{c=1}^{K} q(t_i =c) \\log q(t_i =c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1f772",
   "metadata": {},
   "source": [
    "**Hint**: Split into three following parts:\n",
    "- entropy q from scipy\n",
    "- normal log-prob from scipy\n",
    "- function that collects it all into a sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fddf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vlb(X, pi, mu, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Each input is numpy array:\n",
    "    X: (N x d), data points\n",
    "    gamma: (N x C), distribution q(T)  \n",
    "    pi: (C)\n",
    "    mu: (C x d)\n",
    "    sigma: (C x d x d)\n",
    "    \n",
    "    Returns value of variational lower bound\n",
    "    \"\"\"\n",
    "    N = X.shape[0] # number of objects\n",
    "    C = gamma.shape[1] # number of clusters\n",
    "    d = X.shape[1] # dimension of each object\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e118dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mu, sigma = pi0, mu0, sigma0\n",
    "gamma = E_step(X, pi, mu, sigma)\n",
    "pi, mu, sigma = M_step(X, gamma)\n",
    "loss = compute_vlb(X, pi, mu, sigma, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc7818",
   "metadata": {},
   "source": [
    "### Bringing it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d96cf",
   "metadata": {},
   "source": [
    "Now that we have E step, M step and VLB, we can implement the training loop. We will initialize values of $\\pi$, $\\mu$ and $\\Sigma$ to some random numbers, train until $\\mathcal{L}$ stops changing, and return the resulting points. We also know that the EM algorithm converges to local optima. To find a better local optima, we will restart the algorithm multiple times from different (random) starting positions. Each training trial should stop either when maximum number of iterations is reached or when relative improvement is smaller than given tolerance ($|\\frac{\\mathcal{L}_i-\\mathcal{L}_{i-1}}{\\mathcal{L}_{i-1}}| \\le \\text{rtol}$).\n",
    "\n",
    "Remember, that initial (random) values of $\\pi$ that you generate must be non-negative and sum up to 1. Also, $\\Sigma$ matrices must be symmetric and positive semi-definite. If you don't know how to generate those matrices, you can use $\\Sigma=I$ as initialization.\n",
    "\n",
    "You will also sometimes get numerical errors because of component collapsing. In this case you just need to add 1e-4 to the diagonals.\n",
    "\n",
    "<b>Task 12. <a id=\"task12\"></a> (1.25 points)</b> Implement training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_EM(X, C, rtol=1e-3, max_iter=100, restarts=10):\n",
    "    '''\n",
    "    Starts with random initialization *restarts* times\n",
    "    Runs optimization until saturation with *rtol* reached\n",
    "    or *max_iter* iterations were made.\n",
    "    \n",
    "    X: (N, d), data points\n",
    "    C: int, number of clusters\n",
    "    '''\n",
    "    N = X.shape[0] # number of objects\n",
    "    d = X.shape[1] # dimension of each object\n",
    "    best_loss = None\n",
    "    best_pi = None\n",
    "    best_mu = None\n",
    "    best_sigma = None\n",
    "\n",
    "    for _ in range(restarts):\n",
    "        try:\n",
    "            ### YOUR CODE HERE\n",
    "\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular matrix: components collapsed\")\n",
    "            pass\n",
    "\n",
    "    return best_loss, best_pi, best_mu, best_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss, best_pi, best_mu, best_sigma = train_EM(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9a836",
   "metadata": {},
   "source": [
    "If you implemented all the steps correctly, your algorithm should converge in about 20 iterations. Let's plot the clusters to see it. We will assign a cluster label as the most probable cluster index. This can be found using a matrix $\\gamma$ computed on last E-step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = E_step(X, best_pi, best_mu, best_sigma)\n",
    "labels = gamma.argmax(axis=1)\n",
    "colors = np.array([(31, 119, 180), (255, 127, 14), (44, 160, 44)]) / 255.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors[labels], s=30)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
